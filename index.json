[{"content":"Introduction Pi-hole is a network-wide advertisement and tracker blocker. It\u0026rsquo;s a DNS sinkhole for your home network. I\u0026rsquo;ve been running it for several years now on my Raspberry Pi 3. Recently my SD card died and I didn\u0026rsquo;t have a replacement at hand. So I decided to move Pi-hole to my fanless Intel NUC that I already use to host several services in my home network.\nHere\u0026rsquo;s my log of configuring it with all the problems that I encountered along the way. I decided to first configure it on a freshly installed Fedora 34 VM to iron out all the issues that I may encounter.\nPi-hole has their own container image at https://hub.docker.com/r/pihole/pihole with a quick start guide that uses docker-compose. I don\u0026rsquo;t want to use docker-compose or podman-compose to manage this service. Instead I want to use systemd service to manage pihole container. Compose yaml file looks simple enough to replicate it using podman and systemd.\nPrerequisites Looking at the the prerequisites, we need to expose several ports: 53 TCP/UDP (DNS), 67 IPv4 UDP (DHCP), 547 IPv6 UDP (DHCPv6), 80 TCP (HTTP). I don\u0026rsquo;t want to use this Pi-hole instance to manage my DHCP as I already have it running on my home router, so that leaves only ports 53 and 80.\nPorts First I checked if there is something already running on port 53:\n$ sudo ss -ntpl src :53 State Recv-Q Send-Q Local Address:Port Peer Address:PortProcess LISTEN 0 4096 127.0.0.53%lo:53 0.0.0.0:* users:((\u0026quot;systemd-resolve\u0026quot;,pid=1239,fd=18)) On Fedora since version 33 systemd-resolved is enabled by default and runs a local stub resolver on port 53. To disable it we need to create a drop-in config file /etc/systemd/resolved.conf.d/stub-listener.conf and add the following:\n[Resolve] DNSStubListener=no And restart systemd-resolved:\n$ sudo systemctl restart systemd-resolved Verify that nothing is running on port 53:\n$ sudo ss -ntpl src :53 State Recv-Q Send-Q Local Address:Port Peer Address:PortProcess Same for the port 80. As I didn\u0026rsquo;t have any web servers configured, the output was also empty.\nVolumes Pi-hole uses 2 volumes to store its configuration data: /etc/pihole and /etc/dnsmasq.d. Let\u0026rsquo;s create them first:\n$ podman volume create etc-pihole etc-pihole $ podman volume create etc-dnsmasq.d etc-dnsmasq.d Environment Timezone We need to provide timezone for the container using TZ environment variable. Current timezone on the host can be shown by running timedatectl:\n$ timedatectl show -p Timezone | awk -F= '{print $2}' Web UI password Password for the web UI is set using WEBPASSWD environment variable. If it\u0026rsquo;s not set, the password will be randomly generated.\nDNS According to the documentation, the default upstream DNS provider for Pi-hole is Google. If you want to change it, you can pass DNS1 and DNS2 variables with IP addresses. For example, to use CloudFlare DNS we need to pass these\nDNS1=1.1.1.1 DNS2=1.0.0.1 Podman Now that we have all the required information, we can run the container and see if it works:\n$ podman run --rm -ti \\ -p 53:53/tcp -p 53:53/udp \\ -p 80:80/tcp \\ -e DNS1=1.1.1.1 \\ -e DNS2=1.0.0.1 \\ -e TZ=$(timedatectl show -p Timezone | awk -F= '{print $2}') \\ -e WEBPASSWORD=my_super_secret_password \\ --name pihole docker.io/pihole/pihole:latest It failed for me with an error:\nError: rootlessport cannot expose privileged port 53, you can add 'net.ipv4.ip_unprivileged_port_start=53' to /etc/sysctl.conf (currently 1024), or choose a larger port number (\u0026gt;= 1024): listen tcp 0.0.0.0:53: bind: permission denied It\u0026rsquo;s a known issue with rootless podman:\n Podman can not create containers that bind to ports \u0026lt; 1024.\n The kernel does not allow processes without CAP_NET_BIND_SERVICE to bind to low ports. You can modify the net.ipv4.ip_unprivileged_port_start sysctl to change the lowest port. For example sysctl net.ipv4.ip_unprivileged_port_start=443 allows rootless Podman containers to bind to ports \u0026gt;= 443.   So let\u0026rsquo;s fix this by doing what the error suggests and update the global sysctl configuration by adding net.ipv4.ip_unprivileged_port_start=53 to /etc/sysctl.d/99-sysctl.conf and running\n$ sudo sysctl -p net.ipv4.ip_unprivileged_port_start = 53 Let\u0026rsquo;s run podman command again, this time it succeeds:\ns6-init] making user provided files available at /var/run/s6/etc...exited 0. [s6-init] ensuring user provided files have correct perms...exited 0. [fix-attrs.d] applying ownership \u0026amp; permissions fixes... [fix-attrs.d] 01-resolver-resolv: applying... [fix-attrs.d] 01-resolver-resolv: exited 0. [fix-attrs.d] done. [cont-init.d] executing container initialization scripts... [cont-init.d] 20-start.sh: executing... ::: Starting docker specific checks \u0026amp; setup for docker pihole/pihole [i] Installing configs from /etc/.pihole... [i] Existing dnsmasq.conf found... it is not a Pi-hole file, leaving alone! [âœ“] Installed /etc/dnsmasq.d/01-pihole.conf [âœ“] Installed /etc/dnsmasq.d/06-rfc6761.conf Converting DNS1 to PIHOLE_DNS_ Converting DNS2 to PIHOLE_DNS_ Setting DNS servers based on PIHOLE_DNS_ variable [âœ“] New password set DNSMasq binding to default interface: eth0 Added ENV to php: \u0026quot;PHP_ERROR_LOG\u0026quot; =\u0026gt; \u0026quot;/var/log/lighttpd/error.log\u0026quot;, \u0026quot;ServerIP\u0026quot; =\u0026gt; \u0026quot;0.0.0.0\u0026quot;, \u0026quot;CORS_HOSTS\u0026quot; =\u0026gt; \u0026quot;\u0026quot;, \u0026quot;VIRTUAL_HOST\u0026quot; =\u0026gt; \u0026quot;0.0.0.0\u0026quot;, Using IPv4 and IPv6 ::: setup_blocklists now setting default blocklists up: ::: TIP: Use a docker volume for /etc/pihole/adlists.list if you want to customize for first boot ::: Blocklists (/etc/pihole/adlists.list) now set to: https://raw.githubusercontent.com/StevenBlack/hosts/master/hosts ::: Testing pihole-FTL DNS: FTL started! ::: Testing lighttpd config: Syntax OK ::: All config checks passed, cleared for startup ... ::: Enabling Query Logging [i] Enabling logging... [âœ“] Restarting DNS server [âœ“] Logging has been enabled! ::: Docker start setup complete Checking if custom gravity.db is set in /etc/pihole/pihole-FTL.conf Pi-hole version is v5.5 (Latest: v5.5) AdminLTE version is v5.7 (Latest: v5.7) FTL version is v5.10.2 (Latest: v5.10.2) Container tag is: 2021.10 [cont-init.d] 20-start.sh: exited 0. [cont-init.d] done. [services.d] starting services Starting crond Starting pihole-FTL (no-daemon) as root Starting lighttpd Stopping pihole-FTL [services.d] done. Starting pihole-FTL (no-daemon) as root Stopping pihole-FTL Starting pihole-FTL (no-daemon) as root Troubleshooting Let\u0026rsquo;s see if DNS resolution works:\nOn the host I see that some process is listening on port 53\n$ ss -ntpul src :53 Netid State Recv-Q Send-Q Local Address:Port Peer Address:Port Process udp UNCONN 0 0 *:53 *:* users:((\u0026quot;exe\u0026quot;,pid=8035,fd=15)) tcp LISTEN 0 4096 *:53 *:* users:((\u0026quot;exe\u0026quot;,pid=8035,fd=13)) nslookup doesn\u0026rsquo;t work:\n$ nslookup example.com 127.0.0.1 ;; connection timed out; no servers could be reached But dig works:\ndig +short A example.com 127.0.0.1 93.184.216.34 I\u0026rsquo;ve spent some time playing with tcpdump, but couldn\u0026rsquo;t figure out why this is happening.\nAlthough I noticed one interesting line in the logs from the Pi-hole container:\n DNSMasq binding to default interface: eth0\n It\u0026rsquo;s weird because I don\u0026rsquo;t have this interface inside my container:\n$ podman exec -til /bin/bash root@ddb049b6e786:/# ip a 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: tap0: \u0026lt;BROADCAST,UP,LOWER_UP\u0026gt; mtu 65520 qdisc fq_codel state UNKNOWN group default qlen 1000 link/ether 7a:04:e0:7c:82:3b brd ff:ff:ff:ff:ff:ff inet 10.0.2.100/24 brd 10.0.2.255 scope global tap0 valid_lft forever preferred_lft forever inet6 fe80::7804:e0ff:fe7c:823b/64 scope link valid_lft forever preferred_lft forever Pi-hole accepts INTERFACE environment variable to specify which interface to use. Let\u0026rsquo;s try again, this time using tap0 interface:\n$ podman run --rm -ti \\ -p 53:53/tcp -p 53:53/udp \\ -p 80:80/tcp \\ -e DNS1=1.1.1.1 \\ -e DNS2=1.0.0.1 \\ -e TZ=$(timedatectl show -p Timezone | awk -F= '{print $2}') \\ -e WEBPASSWORD=my_super_secret_password \\ -e INTERFACE=tap0 \\ --name pihole docker.io/pihole/pihole:latest [s6-init] making user provided files available at /var/run/s6/etc...exited 0. [s6-init] ensuring user provided files have correct perms...exited 0. [fix-attrs.d] applying ownership \u0026amp; permissions fixes... [fix-attrs.d] 01-resolver-resolv: applying... [fix-attrs.d] 01-resolver-resolv: exited 0. [fix-attrs.d] done. [cont-init.d] executing container initialization scripts... [cont-init.d] 20-start.sh: executing... ::: Starting docker specific checks \u0026amp; setup for docker pihole/pihole [i] Installing configs from /etc/.pihole... [i] Existing dnsmasq.conf found... it is not a Pi-hole file, leaving alone! [âœ“] Installed /etc/dnsmasq.d/01-pihole.conf [âœ“] Installed /etc/dnsmasq.d/06-rfc6761.conf Converting DNS1 to PIHOLE_DNS_ Converting DNS2 to PIHOLE_DNS_ Setting DNS servers based on PIHOLE_DNS_ variable [âœ“] New password set DNSMasq binding to custom interface: tap0 Added ENV to php: \u0026quot;PHP_ERROR_LOG\u0026quot; =\u0026gt; \u0026quot;/var/log/lighttpd/error.log\u0026quot;, \u0026quot;ServerIP\u0026quot; =\u0026gt; \u0026quot;0.0.0.0\u0026quot;, \u0026quot;CORS_HOSTS\u0026quot; =\u0026gt; \u0026quot;\u0026quot;, \u0026quot;VIRTUAL_HOST\u0026quot; =\u0026gt; \u0026quot;0.0.0.0\u0026quot;, Using IPv4 and IPv6 ::: setup_blocklists now setting default blocklists up: ::: TIP: Use a docker volume for /etc/pihole/adlists.list if you want to customize for first boot ::: Blocklists (/etc/pihole/adlists.list) now set to: https://raw.githubusercontent.com/StevenBlack/hosts/master/hosts ::: Testing pihole-FTL DNS: FTL started! ::: Testing lighttpd config: Syntax OK ::: All config checks passed, cleared for startup ... ::: Enabling Query Logging [i] Enabling logging... [âœ“] Restarting DNS server [âœ“] Logging has been enabled! ::: Docker start setup complete Checking if custom gravity.db is set in /etc/pihole/pihole-FTL.conf Pi-hole version is v5.5 (Latest: v5.5) AdminLTE version is v5.7 (Latest: v5.7) FTL version is v5.10.2 (Latest: v5.10.2) Container tag is: 2021.10 [cont-init.d] 20-start.sh: exited 0. [cont-init.d] done. [services.d] starting services Starting crond Starting pihole-FTL (no-daemon) as root Starting lighttpd Stopping pihole-FTL [services.d] done. Starting pihole-FTL (no-daemon) as root Stopping pihole-FTL Starting pihole-FTL (no-daemon) as root This time interface binds to a correct interface:\n DNSMasq binding to custom interface: tap0\n Let\u0026rsquo;s try nslookup one more time:\n$ nslookup example.com 127.0.0.1 Server: 127.0.0.1 Address: 127.0.0.1#53 Non-authoritative answer: Name: example.com Address: 93.184.216.34 Name: example.com Address: 2606:2800:220:1:248:1893:25c8:1946 It works!\nRunning container as a systemd service Podman has a nice integration with systemd: it can generate a unit file for a container or a pod.\nFirst, we need to create a container. We will reuse the command we used to run podman, but instead of run argument we\u0026rsquo;ll use create:\n$ podman create -ti \\ -p 53:53/tcp -p 53:53/udp \\ -p 80:80/tcp \\ -e DNS1=1.1.1.1 \\ -e DNS2=1.0.0.1 \\ -e TZ=$(timedatectl show -p Timezone | awk -F= '{print $2}') \\ -e WEBPASSWORD=my_super_secret_password \\ -e INTERFACE=tap0 \\ --name pihole docker.io/pihole/pihole:latest c6648518e47aff6b8a8c712fd2abc0a9dc5a426eeb071d76bee595263ba2dff0 $ podman container ls -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES c6648518e47a docker.io/pihole/pihole:latest 12 seconds ago Created 0.0.0.0:53-\u0026gt;53/tcp, 0.0.0.0:53-\u0026gt;53/udp, 0.0.0.0:80-\u0026gt;80/tcp pihole $ podman generate systemd --name pihole # container-pihole.service # autogenerated by Podman 3.4.0 # Fri Oct 8 16:25:44 UTC 2021 [Unit] Description=Podman container-pihole.service Documentation=man:podman-generate-systemd(1) Wants=network-online.target After=network-online.target RequiresMountsFor=/run/user/1000/containers [Service] Environment=PODMAN_SYSTEMD_UNIT=%n Restart=on-failure TimeoutStopSec=70 ExecStart=/usr/bin/podman start pihole ExecStop=/usr/bin/podman stop -t 10 pihole ExecStopPost=/usr/bin/podman stop -t 10 pihole PIDFile=/run/user/1000/containers/overlay-containers/c6648518e47aff6b8a8c712fd2abc0a9dc5a426eeb071d76bee595263ba2dff0/userdata/conmon.pid Type=forking [Install] WantedBy=multi-user.target default.target Save this under $HOME/.config/systemd/user/container-pihole.service and reload systemd:\n$ systemctl --user daemon-reload And start it via systemd:\n$ systemctl --user start container-pihole Let\u0026rsquo;s see if it indeed runs as an unpriviliged container:\n$ ps auxwwf ... fedora 720 0.0 1.5 23404 14948 ? Ss 10:37 0:00 /usr/lib/systemd/systemd --user fedora 722 0.0 0.5 51204 5164 ? S 10:37 0:00 \\_ (sd-pam) fedora 1059 0.0 0.3 18716 3808 ? Ss 11:17 0:00 \\_ /usr/bin/dbus-broker-launch --scope user fedora 1060 0.0 0.1 5044 1180 ? S 11:17 0:00 | \\_ dbus-broker --log 4 --controller 10 --machine-id 2b306968706342eabbbbb5330d2e7c70 --max-bytes 100000000000000 --max-fds 25000000000000 --max-matches 5000000000 fedora 10545 0.0 0.3 5108 3020 ? S 16:27 0:00 \\_ /usr/bin/slirp4netns --disable-host-loopback --mtu=65520 --enable-sandbox --enable-seccomp -c -e 3 -r 4 --netns-type=path /run/user/1000/netns/cni-a629e2c5-fb83-4679-8b0f-3a4e57e03217 tap0 fedora 10547 0.0 4.8 1291228 47952 ? Sl 16:27 0:00 \\_ containers-rootlessport fedora 10553 0.0 4.8 1142100 47788 ? Sl 16:27 0:00 | \\_ containers-rootlessport-child fedora 10561 0.0 0.2 82796 2640 ? Ssl 16:27 0:00 \\_ /usr/bin/conmon --api-version 1 -c c6648518e47aff6b8a8c712fd2abc0a9dc5a426eeb071d76bee595263ba2dff0 -u c6648518e47aff6b8a8c712fd2abc0a9dc5a426eeb071d76bee595263ba2dff0 -r /usr/bin/crun -b /home/fedora/.local/share/containers/storage/overlay-containers/c6648518e47aff6b8a8c712fd2abc0a9dc5a426eeb071d76bee595263ba2dff0/userdata -p /run/user/1000/containers/overlay-containers/c6648518e47aff6b8a8c712fd2abc0a9dc5a426eeb071d76bee595263ba2dff0/userdata/pidfile -n pihole --exit-dir /run/user/1000/libpod/tmp/exits --full-attach -s -l k8s-file:/home/fedora/.local/share/containers/storage/overlay-containers/c6648518e47aff6b8a8c712fd2abc0a9dc5a426eeb071d76bee595263ba2dff0/userdata/ctr.log --log-level warning --runtime-arg --log-format=json --runtime-arg --log --runtime-arg=/run/user/1000/containers/overlay-containers/c6648518e47aff6b8a8c712fd2abc0a9dc5a426eeb071d76bee595263ba2dff0/userdata/oci-log -t --conmon-pidfile /run/user/1000/containers/overlay-containers/c6648518e47aff6b8a8c712fd2abc0a9dc5a426eeb071d76bee595263ba2dff0/userdata/conmon.pid --exit-command /usr/bin/podman --exit-command-arg --root --exit-command-arg /home/fedora/.local/share/containers/storage --exit-command-arg --runroot --exit-command-arg /run/user/1000/containers --exit-command-arg --log-level --exit-command-arg warning --exit-command-arg --cgroup-manager --exit-command-arg systemd --exit-command-arg --tmpdir --exit-command-arg /run/user/1000/libpod/tmp --exit-command-arg --runtime --exit-command-arg crun --exit-command-arg --storage-driver --exit-command-arg overlay --exit-command-arg --events-backend --exit-command-arg journald --exit-command-arg container --exit-command-arg cleanup --exit-command-arg c6648518e47aff6b8a8c712fd2abc0a9dc5a426eeb071d76bee595263ba2dff0 fedora 10564 0.0 0.0 204 4 pts/0 Ss+ 16:27 0:00 \\_ s6-svscan -t0 /var/run/s6/services fedora 10591 0.0 0.0 204 4 pts/0 S+ 16:27 0:00 \\_ s6-supervise s6-fdholderd fedora 10971 0.0 0.0 204 4 pts/0 S+ 16:27 0:00 \\_ s6-supervise cron fedora 10976 0.0 0.2 3740 2860 ? Ss 16:27 0:00 | \\_ bash ./run fedora 10992 0.0 0.2 5516 2412 ? S 16:27 0:00 | \\_ /usr/sbin/cron -f fedora 10972 0.0 0.0 204 4 pts/0 S+ 16:27 0:00 \\_ s6-supervise lighttpd fedora 10978 0.0 0.2 3740 2768 ? Ss 16:27 0:00 | \\_ bash ./run 100032 10985 0.0 0.5 10240 5584 ? S 16:27 0:00 | \\_ lighttpd -D -f /etc/lighttpd/lighttpd.conf 100032 11027 0.0 2.1 197680 21264 ? Ss 16:27 0:00 | \\_ /usr/bin/php-cgi 100032 11028 0.0 0.4 197680 4224 ? S 16:27 0:00 | \\_ /usr/bin/php-cgi 100032 11029 0.0 0.4 197680 4224 ? S 16:27 0:00 | \\_ /usr/bin/php-cgi 100032 11030 0.0 0.4 197680 4224 ? S 16:27 0:00 | \\_ /usr/bin/php-cgi 100032 11031 0.0 0.4 197680 4224 ? S 16:27 0:00 | \\_ /usr/bin/php-cgi fedora 10973 0.0 0.0 204 4 pts/0 S+ 16:27 0:00 \\_ s6-supervise pihole-FTL fedora 11111 0.0 0.3 3740 2968 ? Ss 16:27 0:00 \\_ bash ./run fedora 11116 0.0 0.6 320556 6384 ? Sl 16:27 0:00 \\_ pihole-FTL no-daemon As you can see, none of the components from Pi-hole are running as root.\nConclusion By using podman I was able to run Pi-hole in a rootless unprivileged container as a user systemd service, that is fully controlled by systemd.\nI\u0026rsquo;ve updated DHCP configuration on my router to use newly created Pi-hole instance. I even noticed some speed improvements, especially when I run OpenShift cluster in my lab: OCP generates a lot of DNS traffic that would slow down my RPi. But with Pi-hole running on my Intel NUC I don\u0026rsquo;t see any delays.\n","permalink":"https://vashirov.blog/2021/10/08/running-pi-hole-in-a-rootless-container-using-podman/","summary":"Introduction Pi-hole is a network-wide advertisement and tracker blocker. It\u0026rsquo;s a DNS sinkhole for your home network. I\u0026rsquo;ve been running it for several years now on my Raspberry Pi 3. Recently my SD card died and I didn\u0026rsquo;t have a replacement at hand. So I decided to move Pi-hole to my fanless Intel NUC that I already use to host several services in my home network.\nHere\u0026rsquo;s my log of configuring it with all the problems that I encountered along the way.","title":"Running Pi-hole in a rootless container using podman"},{"content":"Introduction At my homelab I deploy and destroy OpenShift clusters several times a day. One of the most annoying things for me is accessing OpenShift console when it doesn\u0026rsquo;t have proper certificates installed: browser warns about self-signed certificates and makes me do several clicks first in order to access the UI. And of course, in real-life deployments you have to use proper certificates to secure routes and API endpoints.\nThese days it\u0026rsquo;s easy to obtain free TLS certificates using Let\u0026rsquo;s Encrypt or similar services. In March 2018 Let\u0026rsquo;s Encrypt added support for wildcard certificates, that made it finally possible to use it for my OpenShift deployments. I use acme.sh together with DNS API for DNS chanllenge.\nSolution Usually I have 2 different versions of clusters running at the same time: stable and development. So I need to request a wildcard certificate for my main domain plus for api and *.apps subdomains for both clusters:\n$ export DOMAIN=my_domain.fqdn $ acme.sh --issue --dns dns_dreamhost \\  -d \u0026#39;$DOMAIN\u0026#39; \\  -d \u0026#39;*.$DOMAIN\u0026#39; \\  -d \u0026#39;api.stable.$DOMAIN\u0026#39; \\  -d \u0026#39;api.devel.$DOMAIN\u0026#39; \\  -d \u0026#39;*.apps.stable.$DOMAIN\u0026#39; \\  -d \u0026#39;*.apps.devel.$DOMAIN\u0026#39; Installing the certificate Once you sucessfully receive the certificate, it\u0026rsquo;s time to install it. There are 2 places where we need to introduce new certificates: ingress controller and API server.\nIngress controller Create a secret that contains full certificate chain and private key in the openshift-ingress namespace:\n$ oc create secret tls letsencrypt-certs -n openshift-ingress \\  --cert=${HOME}/.acme.sh/${DOMAIN}/fullchain.cer \\  --key=${HOME}/.acme.sh/${DOMAIN}/${DOMAIN}.key \\  --dry-run=client -o yaml | oc apply -f - Then update the ingress controller to use the created secret:\n$ oc patch ingresscontroller default -n openshift-ingress-operator \\  --type=merge --patch=\u0026#39;{\u0026#34;spec\u0026#34;: { \u0026#34;defaultCertificate\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;letsencrypt-certs\u0026#34; }}}\u0026#39; API server Same for the API server: Create a secret that contains full certificate chain and private key in the openshift-config namespace:\n$ oc create secret tls letsencrypt-certs -n openshift-config \\  --cert=${HOME}/.acme.sh/${DOMAIN}/fullchain.cer \\  --key=${HOME}/.acme.sh/${DOMAIN}/${DOMAIN}.key \\  --dry-run=client -o yaml | oc apply -f - And update the API server with the new secret reference:\n$ export CLUSTER=stable $ oc patch apiserver cluster --type=merge \\  -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;servingCerts\u0026#34;: {\u0026#34;namedCertificates\u0026#34;: [{\u0026#34;names\u0026#34;: [\u0026#34;api.\u0026#39;${CLUSTER}\u0026#39;.\u0026#39;${DOMAIN}\u0026#34;], \u0026#34;servingCertificate\u0026#34;: {\u0026#34;name\u0026#34;: \u0026#34;letsencrypt-certs\u0026#34;}}]}}}\u0026#39; It will take some time to rebuild the containers and deploy the pods, but once it\u0026rsquo;s done you would be able to connect to the cluster console without any warnings.\nFixing oc client You might encounter one issue with the CLI tools if they use an old KUBECONFIG. If it references old CA data, you won\u0026rsquo;t be able to connect to the cluster:\n$ oc whoami Unable to connect to the server: x509: certificate signed by unknown authority The solution is to remove the old CA data from your KUBECONFIG:\n$ sed -i -e \u0026quot;s/\\(certificate-authority-data:\\).*//\u0026quot; $KUBECONFIG And after that connection succeeds:\n$ oc whoami system:admin References:  https://docs.openshift.com/container-platform/4.8/security/certificates/replacing-default-ingress-certificate.html https://docs.openshift.com/container-platform/4.8/security/certificates/api-server.html  ","permalink":"https://vashirov.blog/2021/10/04/installing-lets-encrypt-certificates-in-openshift/","summary":"Introduction At my homelab I deploy and destroy OpenShift clusters several times a day. One of the most annoying things for me is accessing OpenShift console when it doesn\u0026rsquo;t have proper certificates installed: browser warns about self-signed certificates and makes me do several clicks first in order to access the UI. And of course, in real-life deployments you have to use proper certificates to secure routes and API endpoints.\nThese days it\u0026rsquo;s easy to obtain free TLS certificates using Let\u0026rsquo;s Encrypt or similar services.","title":"Installing Letâ€™s Encrypt certificates in OpenShift"},{"content":"I was pleasantly surprised to find out that oc (and kubectl too) has a built-in capability to generate shell auto completion.\nFor Zsh add the following snippet to your ~/.zshrc:\nif [ $commands[oc] ]; then source \u0026lt;(oc completion zsh) fi For Bash add this to ~/.bashrc:\nif command -v oc \u0026amp;\u0026gt;/dev/null; then source \u0026lt;(oc completion bash) fi After opening a new shell:\n$ oc \u0026lt;TAB\u0026gt; adm describe logout replace annotate diff logs rollback api-resources edit new-app rollout api-versions ex new-build rsh apply exec new-project rsync attach explain observe run auth expose options scale autoscale extract patch secrets cancel-build get plugin serviceaccounts cluster-info help policy set completion idle port-forward start-build config image process status cp import-image project tag create kustomize projects version debug label proxy wait delete login registry whoami ","permalink":"https://vashirov.blog/2021/04/10/how-to-enable-shell-auto-completion-for-openshift-oc-command/","summary":"I was pleasantly surprised to find out that oc (and kubectl too) has a built-in capability to generate shell auto completion.\nFor Zsh add the following snippet to your ~/.zshrc:\nif [ $commands[oc] ]; then source \u0026lt;(oc completion zsh) fi For Bash add this to ~/.bashrc:\nif command -v oc \u0026amp;\u0026gt;/dev/null; then source \u0026lt;(oc completion bash) fi After opening a new shell:\n$ oc \u0026lt;TAB\u0026gt; adm describe logout replace annotate diff logs rollback api-resources edit new-app rollout api-versions ex new-build rsh apply exec new-project rsync attach explain observe run auth expose options scale autoscale extract patch secrets cancel-build get plugin serviceaccounts cluster-info help policy set completion idle port-forward start-build config image process status cp import-image project tag create kustomize projects version debug label proxy wait delete login registry whoami ","title":"How to enable shell auto completion for OpenShift `oc` command"},{"content":"ðŸ‘‹ ","permalink":"https://vashirov.blog/1970/01/01/hello-world/","summary":"ðŸ‘‹ ","title":"Hello World!"}]